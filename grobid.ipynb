{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33938d66-d73e-4272-8234-7de66da040a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grobid.client import GrobidClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0481b73-a5d4-4761-9233-c0d77680d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = GrobidClient(port='8070')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "064483d2-3dfd-41f6-85aa-0ce27739da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp = client.serve(service='processFulltextDocument',\n",
    "                   pdf_file='new_data/2023.alp-1.1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e24ec54-4f92-4334-972d-0874ca5bc04a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\" \\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \\nxsi:schemaLocation=\"http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd\"\\n xmlns:xlink=\"http://www.w3.org/1999/xlink\">\\n\\t<teiHeader xml:lang=\"en\">\\n\\t\\t<fileDesc>\\n\\t\\t\\t<titleStmt>\\n\\t\\t\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_ZcuYkH8\">Training and Evaluation of Named Entity Recognition Models for Classical Latin</title>\\n\\t\\t\\t</titleStmt>\\n\\t\\t\\t<publicationStmt>\\n\\t\\t\\t\\t<publisher/>\\n\\t\\t\\t\\t<availability status=\"unknown\"><licence/></availability>\\n\\t\\t\\t</publicationStmt>\\n\\t\\t\\t<sourceDesc>\\n\\t\\t\\t\\t<biblStruct>\\n\\t\\t\\t\\t\\t<analytic>\\n\\t\\t\\t\\t\\t\\t<author>\\n\\t\\t\\t\\t\\t\\t\\t<persName coords=\"1,66.19,109.19,100.48,10.75\"><forename type=\"first\">Marijke</forename><surname>Beersmans</surname></persName>\\n\\t\\t\\t\\t\\t\\t\\t<email>marijke.beersmans@kuleuven.be</email>\\n\\t\\t\\t\\t\\t\\t\\t<affiliation key=\"aff0\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<orgName type=\"department\">Faculty of Arts Blijde Inkomststraat 21</orgName>\\n\\t\\t\\t\\t\\t\\t\\t\\t<orgName type=\"institution\">KU Leuven</orgName>\\n\\t\\t\\t\\t\\t\\t\\t\\t<address>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<postCode>3000</postCode>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<settlement>Leuven</settlement>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<country key=\"BE\">Belgium</country>\\n\\t\\t\\t\\t\\t\\t\\t\\t</address>\\n\\t\\t\\t\\t\\t\\t\\t</affiliation>\\n\\t\\t\\t\\t\\t\\t</author>\\n\\t\\t\\t\\t\\t\\t<author>\\n\\t\\t\\t\\t\\t\\t\\t<persName coords=\"1,195.89,109.19,86.22,10.75\"><forename type=\"first\">Evelien</forename><surname>De Graaf</surname></persName>\\n\\t\\t\\t\\t\\t\\t\\t<email>evelien.degraaf@kuleuven.be</email>\\n\\t\\t\\t\\t\\t\\t\\t<affiliation key=\"aff0\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<orgName type=\"department\">Faculty of Arts Blijde Inkomststraat 21</orgName>\\n\\t\\t\\t\\t\\t\\t\\t\\t<orgName type=\"institution\">KU Leuven</orgName>\\n\\t\\t\\t\\t\\t\\t\\t\\t<address>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<postCode>3000</postCode>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<settlement>Leuven</settlement>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<country key=\"BE\">Belgium</country>\\n\\t\\t\\t\\t\\t\\t\\t\\t</address>\\n\\t\\t\\t\\t\\t\\t\\t</affiliation>\\n\\t\\t\\t\\t\\t\\t</author>\\n\\t\\t\\t\\t\\t\\t<author>\\n\\t\\t\\t\\t\\t\\t\\t<persName coords=\"1,311.33,109.19,93.33,10.75\"><forename type=\"first\">Tim</forename><surname>Van De Cruys</surname></persName>\\n\\t\\t\\t\\t\\t\\t\\t<email>tim.vandecruys@kuleuven.be</email>\\n\\t\\t\\t\\t\\t\\t\\t<affiliation key=\"aff0\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<orgName type=\"department\">Faculty of Arts Blijde Inkomststraat 21</orgName>\\n\\t\\t\\t\\t\\t\\t\\t\\t<orgName type=\"institution\">KU Leuven</orgName>\\n\\t\\t\\t\\t\\t\\t\\t\\t<address>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<postCode>3000</postCode>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<settlement>Leuven</settlement>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<country key=\"BE\">Belgium</country>\\n\\t\\t\\t\\t\\t\\t\\t\\t</address>\\n\\t\\t\\t\\t\\t\\t\\t</affiliation>\\n\\t\\t\\t\\t\\t\\t</author>\\n\\t\\t\\t\\t\\t\\t<author>\\n\\t\\t\\t\\t\\t\\t\\t<persName coords=\"1,433.88,109.19,98.20,10.75\"><forename type=\"first\">Margherita</forename><surname>Fantoli</surname></persName>\\n\\t\\t\\t\\t\\t\\t\\t<email>margherita.fantoli@kuleuven.be</email>\\n\\t\\t\\t\\t\\t\\t\\t<affiliation key=\"aff0\">\\n\\t\\t\\t\\t\\t\\t\\t\\t<orgName type=\"department\">Faculty of Arts Blijde Inkomststraat 21</orgName>\\n\\t\\t\\t\\t\\t\\t\\t\\t<orgName type=\"institution\">KU Leuven</orgName>\\n\\t\\t\\t\\t\\t\\t\\t\\t<address>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<postCode>3000</postCode>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<settlement>Leuven</settlement>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t<country key=\"BE\">Belgium</country>\\n\\t\\t\\t\\t\\t\\t\\t\\t</address>\\n\\t\\t\\t\\t\\t\\t\\t</affiliation>\\n\\t\\t\\t\\t\\t\\t</author>\\n\\t\\t\\t\\t\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_4fhckE4\">Training and Evaluation of Named Entity Recognition Models for Classical Latin</title>\\n\\t\\t\\t\\t\\t</analytic>\\n\\t\\t\\t\\t\\t<monogr>\\n\\t\\t\\t\\t\\t\\t<imprint>\\n\\t\\t\\t\\t\\t\\t\\t<date/>\\n\\t\\t\\t\\t\\t\\t</imprint>\\n\\t\\t\\t\\t\\t</monogr>\\n\\t\\t\\t\\t\\t<idno type=\"MD5\">9A8F537C193B04C64A940A12BB01A8CF</idno>\\n\\t\\t\\t\\t\\t<idno type=\"DOI\">10.26615/978-954-452-087-8_001</idno>\\n\\t\\t\\t\\t</biblStruct>\\n\\t\\t\\t</sourceDesc>\\n\\t\\t</fileDesc>\\n\\t\\t<encodingDesc>\\n\\t\\t\\t<appInfo>\\n\\t\\t\\t\\t<application version=\"0.8.1\" ident=\"GROBID\" when=\"2024-11-29T14:33+0000\">\\n\\t\\t\\t\\t\\t<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>\\n\\t\\t\\t\\t\\t<ref target=\"https://github.com/kermitt2/grobid\"/>\\n\\t\\t\\t\\t</application>\\n\\t\\t\\t</appInfo>\\n\\t\\t</encodingDesc>\\n\\t\\t<profileDesc>\\n\\t\\t\\t<abstract>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"_8nDRNbC\"><p xml:id=\"_xZnxR68\">We evaluate the performance of various models on the task of named entity recognition (NER) for classical Latin. Using an existing dataset, we train two transformer-based Latin-BERT models and one shallow conditional random field (CRF) model. The performance is assessed using both standard metrics and a detailed manual error analysis, and compared to the results obtained by different already released Latin NER tools. Both analyses demonstrate that the BERT models achieve a better f1-score than the other models. Furthermore, we annotate new, unseen data for further evaluation of the models, and we discuss the impact of annotation choices on the results.</p></div>\\n\\t\\t\\t</abstract>\\n\\t\\t</profileDesc>\\n\\t</teiHeader>\\n\\t<facsimile>\\n\\t\\t<surface n=\"1\" ulx=\"0.0\" uly=\"0.0\" lrx=\"595.276\" lry=\"841.89\"/>\\n\\t\\t<surface n=\"2\" ulx=\"0.0\" uly=\"0.0\" lrx=\"595.276\" lry=\"841.89\"/>\\n\\t\\t<surface n=\"3\" ulx=\"0.0\" uly=\"0.0\" lrx=\"595.276\" lry=\"841.89\"/>\\n\\t\\t<surface n=\"4\" ulx=\"0.0\" uly=\"0.0\" lrx=\"595.276\" lry=\"841.89\"/>\\n\\t\\t<surface n=\"5\" ulx=\"0.0\" uly=\"0.0\" lrx=\"595.276\" lry=\"841.89\"/>\\n\\t\\t<surface n=\"6\" ulx=\"0.0\" uly=\"0.0\" lrx=\"595.276\" lry=\"841.89\"/>\\n\\t\\t<surface n=\"7\" ulx=\"0.0\" uly=\"0.0\" lrx=\"595.276\" lry=\"841.89\"/>\\n\\t\\t<surface n=\"8\" ulx=\"0.0\" uly=\"0.0\" lrx=\"595.276\" lry=\"841.89\"/>\\n\\t\\t<surface n=\"9\" ulx=\"0.0\" uly=\"0.0\" lrx=\"595.276\" lry=\"841.89\"/>\\n\\t\\t<surface n=\"10\" ulx=\"0.0\" uly=\"0.0\" lrx=\"595.276\" lry=\"841.89\"/>\\n\\t\\t<surface n=\"11\" ulx=\"0.0\" uly=\"0.0\" lrx=\"595.276\" lry=\"841.89\"/>\\n\\t\\t<surface n=\"12\" ulx=\"0.0\" uly=\"0.0\" lrx=\"595.276\" lry=\"841.89\"/>\\n\\t</facsimile>\\n\\t<text xml:lang=\"en\">\\n\\t\\t<body>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"1\" xml:id=\"_3haSUsN\">Introduction</head><p xml:id=\"_GtYVeGb\">Commonly an important precursor to information extraction, text summarisation and the creation of knowledge bases, Named Entity Recognition (NER) has become a ubiquitous task in Natural Language Processing (NLP). For modern highresource languages, generic NER off-the-shelf solutions, focusing mainly on identifying locations, organizations and people, can produce highly accurate annotations. For historical languages, even prolific ones like Latin, the task remains a challenge, in part due to a lack of annotated corpora and tools <ref type=\"bibr\" coords=\"1,113.29,592.97,96.50,9.46\" target=\"#b7\">(Ehrmann et al., 2021)</ref>.</p><p xml:id=\"_Z2neuDg\">We pursue three main objectives with this paper:</p><p xml:id=\"_fgh2G3Y\">\\xe2\\x80\\xa2 We compare the performance of three different models for Latin NER using pre-existing, openly available data. The comparison is both quantitative and qualitative.</p><p xml:id=\"_4SDKMjg\">\\xe2\\x80\\xa2 Based on the analysis of existing annotations and the results of automatic annotation, we publish a new set of gold data, providing documentation of the most critical choices.</p><p xml:id=\"_y4hSC9a\">\\xe2\\x80\\xa2 By using the newly annotated data to assess the results of NER, we publish the automatic annotation by the best-performing model of a large corpus of literary classical Latin texts and documenting the strengths and weaknesses of the resulting annotation.</p><p xml:id=\"_pWUc2Z5\">The paper contributes to the application of NLP to Latin on a methodological level, since we propose a thorough analysis of the results of NER on Latin and identify the most critical points. In addition, the paper is associated with the publication of NER models and datasets, and documents the choices that have been implemented. The paper is structured as follows: after introducing existing work and datasets related to NER for Classical Languages (Section 2), we describe the data used, and the training of the models and their performance on in-domain and out-of-domain test sets (Section 3).</p><p xml:id=\"_e77wmqb\">Section 4 provides a qualitative error analysis of the best performing model based on F1 metrics. In section 5, we introduce the annotation of new data from the LASLA corpus, and analyse the results of the automatic annotation by the best-performing model. The data and code related to this paper are made available on a Github repository.<ref type=\"foot\" coords=\"1,474.61,522.61,3.99,6.91\" target=\"#foot_0\">foot_0</ref> </p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"2\" xml:id=\"_kqFmqqv\">Related work</head><p xml:id=\"_3VyXMKa\">Previous work has highlighted the challenges linked to NER for Latin. <ref type=\"bibr\" coords=\"1,424.85,584.58,100.29,9.46\" target=\"#b7\">Ehrmann et al. (2021)</ref> identified among others the following relevant challenges concerning NER on historical documents: variable and sparse feature space (generalizing over different genres and domains, cf. <ref type=\"bibr\" coords=\"1,459.99,638.78,66.33,9.46;1,305.78,652.33,27.11,9.46\" target=\"#b9\">Erdmann et al. (2016)</ref>), dynamics of language such as spelling variations and change in naming conventions, general lack of resources (e.g. typologies from modern languages not fitting for historical documents).</p><p xml:id=\"_kDcAp3t\">In addition, Burns (2023) underlined another difficulty of the already scarce resources: differences in orthographic conventions and annotation schemes. Lastly, both <ref type=\"bibr\" coords=\"2,171.20,74.72,98.88,9.46\" target=\"#b3\">Chastang et al. (2021)</ref> and Torres Aguilar (2022) consider the frequency of overlapped and nested entities in Latin as a challenge.</p><p xml:id=\"_Fsc3YvK\">When it comes to existing models, <ref type=\"bibr\" coords=\"2,247.72,132.83,41.42,9.46;2,70.87,146.38,53.87,9.46\" target=\"#b3\">Chastang et al. (2021)</ref> trained a CRF-based model on Latin medieval charters from Burgundy. Later Torres Aguilar (2022) tested two approaches for creating a multilingual pipeline for medieval charters (French, Spanish and Latin): the first uses contextual and static embeddings coupled to a BiLSTM-CRF (Bidirectional Long-Short Term Memory) classifier, and the second employs a fine-tuning method using the pre-trained multilingual BERT and RoBERTa models. For both of these efforts, custom charter corpora were annotated. In the context of the Herodotos project -which aims to catalogue ancient ethno-political groups and their interactions - <ref type=\"bibr\" coords=\"2,136.28,322.52,87.60,9.46\" target=\"#b9\">Erdmann et al. (2016</ref><ref type=\"bibr\" coords=\"2,230.90,322.52,19.95,9.46\" target=\"#b10\">Erdmann et al. ( , 2019</ref>) created a neural, BiLSTM-CRF based entity recognizer <ref type=\"bibr\" coords=\"2,70.51,349.62,98.20,9.46\" target=\"#b15\">(Lample et al., 2016)</ref> trained on classical Latin texts. In addition, NER is included in text analysis pipelines for Latin, such as the Classical Language Toolkit (CLTK; <ref type=\"bibr\" coords=\"2,143.39,390.27,89.71,9.46\" target=\"#b13\">Johnson et al., 2021)</ref> and LatinCy <ref type=\"bibr\" coords=\"2,70.51,403.82,58.63,9.46\" target=\"#b1\">(Burns, 2023)</ref>.</p><p xml:id=\"_XqQ2N8Z\">In recent years, transformer-based models (with the BERT architecture as one of the prime instantiations) have become the norm for various NLP applications <ref type=\"bibr\" coords=\"2,124.98,461.93,95.22,9.46\" target=\"#b8\">(Ehrmann et al., 2022;</ref><ref type=\"bibr\" coords=\"2,222.65,461.93,67.85,9.46;2,70.87,475.48,25.35,9.46\" target=\"#b24\">Sprugnoli et al., 2022;</ref><ref type=\"bibr\" coords=\"2,99.94,475.48,126.02,9.46\">Sommerschield et al., 2023)</ref>. These models have been leveraged, inter alia, for Latin morphosyntactic tagging <ref type=\"bibr\" coords=\"2,167.11,502.57,122.93,9.46\" target=\"#b27\">(Wr\\xc3\\xb3bel and Nowak, 2022;</ref><ref type=\"bibr\" coords=\"2,70.87,516.12,146.98,9.46\" target=\"#b16\">Mercelis and Keersmaekers, 2022;</ref><ref type=\"bibr\" coords=\"2,220.15,516.12,69.71,9.46\" target=\"#b17\">Nehrdich, 2022)</ref> and translation alignment for ancient languages <ref type=\"bibr\" coords=\"2,70.51,543.22,94.73,9.46\">(Yousef et al., 2022b)</ref>, which could also be leveraged for named entity projection from modern languages given a parallel corpus <ref type=\"bibr\" coords=\"2,201.83,570.32,84.61,9.46\" target=\"#b28\">(Yousef et al., 2023)</ref>. For Greek NER, a BERT-based approach equally proved to be effective <ref type=\"bibr\" coords=\"2,166.25,597.42,89.94,9.46\">(Yousef et al., 2022a)</ref>. There already exists a transformer-based model for Latin (LatinBERT; <ref type=\"bibr\" coords=\"2,135.67,624.52,123.11,9.46\" target=\"#b0\">Bamman and Burns, 2020)</ref> but to the best of our knowledge, it has not yet been finetuned on the task of named entity recognition.</p><p xml:id=\"_Y8eutyD\">Regarding datasets, the Herodotos dataset (at the time of training) is the only available NER dataset for classical Latin <ref type=\"bibr\" coords=\"2,189.30,696.18,96.18,9.46\" target=\"#b10\">(Erdmann et al., 2019</ref><ref type=\"bibr\" coords=\"2,70.87,709.73,23.95,9.46\">(Erdmann et al., , 2023))</ref>. Additionally, the authors of the LatinCy pipeline are planning to make their custom dataset publicly available <ref type=\"bibr\" coords=\"2,152.62,736.82,59.81,9.46\" target=\"#b1\">(Burns, 2023)</ref>. Lastly, the multilingual Medieval charter dataset, which includes non-classical Latin <ref type=\"bibr\" coords=\"2,155.01,763.92,94.75,9.46\" target=\"#b25\">(Torres Aguilar, 2022)</ref> 3 Data and methods</p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.1\" xml:id=\"_bAXSvHY\">Data</head><p xml:id=\"_YWgepQv\">The Herodotos dataset contains two full texts, Caesar\\'s Bellum Gallicum (BGall.) and Ovid\\'s Ars Amatoria (Ars am.), and excerpts from three other texts: a part of the first book of Caesar\\'s Bellum Civile (BCiv.); book 1, book 2 and a part of book 3 of Pliny the Younger\\'s Epistulae (Ep.); the preface, first and a part of the second book of Pliny the Elder\\'s Naturalis Historia (NH). The editions were taken from the Latin Library <ref type=\"bibr\" coords=\"2,453.13,421.98,52.52,9.46\">(Carey, s.d.)</ref> and the Perseus Project <ref type=\"bibr\" coords=\"2,395.48,435.53,86.10,9.46\" target=\"#b22\">(Smith et al., 2000)</ref>. Table <ref type=\"table\" coords=\"2,519.66,435.53,5.56,9.46\">1</ref> contains an overview of the dataset sizes.</p><p xml:id=\"_TapKJux\">The texts are manually annotated for location (\\'LOC\\'), person (\\'PERS\\') and (socio-ethnic) group (\\'GRP\\') entities <ref type=\"bibr\" coords=\"2,381.11,489.92,100.16,9.46\" target=\"#b9\">(Erdmann et al., 2016)</ref>. The annotations are encoded in BIO-format, where each token is mapped to an \\'O\\' (for \\'outside\\', not an entity) or an entity type with either a B-or an I-prefix. The B-prefix, for \\'beginning\\', indicates the first or only word of an entity whereas the I-prefix, for \\'inside\\', specifies a continuation of a multi-word entity. Nested entities were not considered.</p><p xml:id=\"_pbTMgJf\">On the whole dataset, minimal preprocessing was performed to iron out formatting mistakes. Afterwards, the five works were divided into two parts: in-domain, used for training and in-domain testing, and out-domain, used exclusively for out-domain testing. The latter should assess the model\\'s generalizing capabilities to texts that are significantly different from the data it was trained on. In this experiment, the in-domain part consisted of the prose texts, <ref type=\"bibr\" coords=\"2,331.67,720.26,121.09,9.64\">(BGall., Bciv., Ep. and NH.)</ref>  The in-domain texts were then split into three sets: a training set (75%), a validation set (12.5%) and an in-domain test set (12.5%). As the BERTmodel processes input on the sentence level, the sentence order was randomized. The sentences containing rare multi-word locations and groups were identified and split separately. Each of those splits was later appended to one of the three sets to ensure that each contained entities of every type. The frequencies of the entity types can be found in Table <ref type=\"table\" coords=\"3,111.54,397.20,5.56,9.46\">2</ref> (train and validation split) and in the \\'support\\' column of Table <ref type=\"table\" coords=\"3,185.74,410.75,5.45,9.46\" target=\"#tab_4\">5</ref> (test split).</p><p xml:id=\"_Cjyrssq\">To ensure representative testing, the data was augmented with manually annotated test sets from the LASLA corpus in the second part of this paper (see Section 5), both for in-domain prose and outdomain poetry.</p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2\" xml:id=\"_PU8VGWB\">Model training and evaluation</head><p xml:id=\"_wnPGWkm\">We created two models on the Herodotos dataset and compared the results of these models to those obtained using the recently released LatinCy toolkit. The models we trained (finetuned) ourselves are:</p><p xml:id=\"_AWUn7x4\">\\xe2\\x80\\xa2 A conditional random field (CRF) model. Erdmann et al. ( <ref type=\"formula\" coords=\"3,148.78,606.59,19.54,9.46\">2016</ref>) use a CRF-based baseline in a similar context. This model is fairly simple and will serve as a starting point for comparison.</p><p xml:id=\"_scVANjw\">\\xe2\\x80\\xa2 LatinBERT <ref type=\"bibr\" coords=\"3,148.31,669.08,126.33,9.46\" target=\"#b0\">(Bamman and Burns, 2020)</ref>, a specialized BERT model for Latin, trained using the Masked Language Modeling objective on a corpus of 642.7M words, ranging from classical Latin (from 200 BCE onwards) to Neolatin from Wikipedia. We made use of the pre-trained model, and finetuned it on the NER dataset.</p><p xml:id=\"_yTQp2mH\">The results of these models are compared to Lat-inCy, a SpaCy pipeline for Latin, and for the LASLA test set (see below) to the Herodotos entity recognizer <ref type=\"bibr\" coords=\"3,373.68,115.37,100.69,9.46\" target=\"#b9\">(Erdmann et al., 2016)</ref> as well. In order to train several SpaCy pipelines <ref type=\"bibr\" coords=\"3,479.29,128.92,45.12,9.46;3,306.14,142.47,84.10,9.46\">(Honnibal and Montani, 2017)</ref> for Latin (viz. a small, medium and large model), Burns (2023) leveraged the five Latin Universal Dependencies treebanks and several large Latin corpora. LatinCy\\'s named entity recognizers were trained separately from the rest of their respective pipelines, on a custom-made dataset based on the UD treebanks and the dataset of the Herodotos project. For this paper, we tested the large (\\'la_core_web_lg\\') pipeline, as well as the \\'la_core_web_trf\\' pipeline, which is backed by the multilingual BERT transformer architecture <ref type=\"bibr\" coords=\"3,305.78,291.51,85.92,9.46\" target=\"#b6\">(Devlin et al., 2018)</ref>.</p><p xml:id=\"_9mSJF2t\">The next two subsections describe the training setup for our models; section 3.3 discusses the results of the models we trained, as well as a comparison to LatinCy\\'s performance.</p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2.1\" xml:id=\"_J2BSw6R\">CRF</head><p xml:id=\"_3UFEU5v\">For the CRF model, we made use of an implementation based on CRFsuite <ref type=\"bibr\" coords=\"3,408.68,399.00,67.49,9.46\" target=\"#b18\">(Okazaki, 2007)</ref>. We specified the optimization method as l-bfgs, set the maximum number of iterations to 100 and considered all possible transitions, The following hand-crafted features are incorporated: whether the word is a digit, capitalised or fully upper-cased; whether the word is the first or last word of a sentence; the last three letters; the last two letters; a context window of two left words and two right words. Following <ref type=\"bibr\" coords=\"3,306.14,520.94,96.82,9.46\" target=\"#b19\">Palladino et al. (2020)</ref>, the whole word itself was not included, because this might aid generalization to other contexts.</p><p xml:id=\"_tmGYPjD\">Hyperparameter optimization was performed using a 50-fold random search, to optimize the two regularisation coefficients c1 (search space exponentially distributed on scale 0.5) and c2 (search space exponentially distributed on scale 0.05). The best hyperparameters were 0.183 and 0.086 for c1 and c2 respectively.</p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.2.2\" xml:id=\"_2ThAjKW\">LatinBERT</head><p xml:id=\"_25WXkBG\">Prior to the finetuning of LatinBERT, we incorporated the original subword tokenizer into our own, custom tokenizer to ensure the model was fully compatible with the transformers library <ref type=\"bibr\" coords=\"3,498.71,723.27,25.69,9.46;3,306.14,736.82,52.46,9.46\" target=\"#b26\">(Wolf et al., 2020)</ref>. All words were lowercased during tokenization. We proceeded to utilize the transformers trainer API both with and without hyperpa-</p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head xml:id=\"_v74rv9u\">Hyperpar. Initial Optimized</head><p xml:id=\"_W99E8BB\">Learning rate 2.00e-5 7.89e-5 Weight decay 0.01 0.10 Number of train epochs 3 3 rameter optimization (results reported under Latin-BERT2 and LatinBERT1 respectively). During the experiments with hyperparameter optimization, we specified the optimization method as random. The metric for evaluation is the validation loss, and the goal is to minimize it based on a ten-fold search.</p><p xml:id=\"_Y9aeGQR\">Table <ref type=\"table\" coords=\"4,96.72,276.69,5.35,9.46\" target=\"#tab_2\">3</ref> provides a comparison of the hyperparameters used. In both cases the per-device train batch size is 16 and the warmup ratio is 0.1.</p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"3.3\" xml:id=\"_CWnKfjF\">Results</head><p xml:id=\"_wBbaQu9\">In Table <ref type=\"table\" coords=\"4,113.18,343.86,5.56,9.46\" target=\"#tab_3\">4</ref> we report the micro-averaged f1 (or accuracy) based on the token labeling. The microaveraged f1 computes the proportion of correctly classified observations out of all observations. In Table <ref type=\"table\" coords=\"4,99.56,398.06,4.17,9.46\" target=\"#tab_4\">5</ref>, for every entity type (\\'PERS\\', \\'LOC\\', \\'GRP\\'), we report the f1 score (harmonic mean of precision and recall) on the entity level, where the full entity is only considered correct if the annotations for all its comprising tokens match the gold standard exactly, and the macro f1, where the results for each model are averaged across the various labels without taking class size into account.</p><p xml:id=\"_ah88fsk\">In Appendix A, more detailed counts per label are provided (Table <ref type=\"table\" coords=\"4,142.54,520.00,8.63,9.46\">10</ref>).</p><p xml:id=\"_6UGXbSd\">The overall results in Table <ref type=\"table\" coords=\"4,210.32,533.59,5.56,9.46\" target=\"#tab_3\">4</ref> show that there is a drop in performance going from in-to out-ofdomain, signaling a difficulty to generalize from prose to poetry. Both LatinBERTs outperform the other models in-and out-of-domain. However, it is important to note that optimizing the hyperparameters causes a slight increase in macro-f1 on the in-domain dataset, but a symmetrical, decrease on the out-of-domain dataset. Looking at the entity level metrics in Table <ref type=\"table\" coords=\"4,166.52,655.53,4.06,9.46\" target=\"#tab_4\">5</ref>, \\'PERS\\' is the class that is the easiest to predict for every model. For the models exclusively trained on the Herodotos data (the CRF and LatinBERTs), single word groups are a relatively well-understood category in-domain, but cause problems out-of-domain. Unfortunately, no multi-token \\'GRP\\' were correctly detected, which can be explained by their rarity. Multi-token \\'LOC\\' are also rarely detected, with only the BERT mod-els being able to recognize some in-domain (See again Table <ref type=\"table\" coords=\"4,359.14,88.27,8.63,9.46\">10</ref>).</p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4\" xml:id=\"_EtrtwQy\">Error analysis 4.1 Ambiguous annotations in the training data</head><p xml:id=\"_FbMBKhn\">Although guidelines for named entities in classical scholarship exist <ref type=\"bibr\" coords=\"4,384.91,177.01,140.86,9.46;4,306.14,190.56,23.95,9.46\" target=\"#b21\">(Romanello and Najem-Meyer, 2022)</ref>, for classical Latin texts, they are still lacking (see Section 5). This is reflected in our dataset.</p><p xml:id=\"_SjQJDVb\">We can hypothesize that this impacts the overall performance of the models. In particular, some tokens are annotated as different entities throughout the dataset. In some cases, this is due to the inherent ambiguity of the token, as in the following examples:</p><p xml:id=\"_SxY2kWA\">\\xe2\\x80\\xa2 Homonyms: Galli (genitive singular of \\'Gallus\\', name of a man) as \\'PERS\\' in Ars am.</p><p xml:id=\"_zBYhf8H\">3.334 or \\'GRP\\' in BGall. 1.1 (\\'the Gauls\\');</p><p xml:id=\"_v2nEFKm\">\\xe2\\x80\\xa2 Tokens that occur both as entity and nonentity in the dataset: e.g. Liber (a divinity, but also \\'book\\'), forms of Sol (divinity \\'Sun\\' and the sun), and Gratia (\\'grace\\', but also the divinity \\'Grace\\') appear both as entities (personifications, usually capitalized) and nonentities (regular use);</p><p xml:id=\"_Gkq5BJk\">\\xe2\\x80\\xa2 Patronyms such as Atrides (\\'descendant of Atreus\\'): sometimes forms of these refer to one specific person, sometimes to a group.</p><p xml:id=\"_cvG8TnP\">In other cases, the differences seem to stem from inconsistent annotation choices:</p><p xml:id=\"_UWeS8pP\">\\xe2\\x80\\xa2 Multi-token entities that contain a toponym: e.g. the entity Amphilocho Athenaeo prouincia (\\'province\\') and terra (\\'region\\') are annotated as \\'LOC\\', and some of the occurrences of equestri and praetori as \\'GRP\\'.</p><p xml:id=\"_btMqRtY\">In addition, entire parts of text are not annotated in Ars am. and NH. The scarcity of data also appears to be a problem: out of the 180 unique tokens that were not correctly identified by any model, 132 do not occur in the training data.</p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"4.2\" xml:id=\"_AjqP8Vm\">Qualitative analysis LatinBERT</head><p xml:id=\"_pKEvRRx\">In this section, we perform a qualitative error analysis of the performance of the two best-performing models, LatinBERT1 and 2, on both the in-domain and out-of-domain sets, in order to better understand the origin of the errors. First, LatinBERT1 and LatinBERT2 share common issues, that are generally not encountered by at least one of the other two models: LatinBERT1 and LatinBERT2 differ only in the optimization of the hyperparameters, which seems nonetheless to have a relevant impact on the performance. In a total of 223 cases, the prediction of LatinBERT2 differs from LatinBERT1. Table <ref type=\"table\" coords=\"5,518.97,655.53,5.43,9.46\" target=\"#tab_8\">8</ref> in Appendix A shows that LatinBERT1 slightly outperforms LatinBERT2 on the label \\'B-PERS\\'. However, in several cases, the prediction of Latin-BERT2 classifies the category correctly but with wrong segmentation, predicting \\'I-PERS\\' instead of \\'B-PERS\\', whereas LatinBERT1 also classifies incorrectly. In 46 of the cases where only Latin-BERT1 is correct, LatinBERT2 predicts a non-entity. 42 of these tokens did not appear in the train or validation set and the others were either annotated both as entities and \\'O\\' or appeared only once in the training data. Besides this, many differences can be explained by the difficulties in \\'GRP\\'/\\'LOC\\' distinction identified in Section 4.1.</p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5\" xml:id=\"_ey5Rx8s\">Annotation of the LASLA corpus</head><p xml:id=\"_qyKsaSU\">In what follows, we discuss the performance of the same NER models on the LASLA Latin corpus. <ref type=\"foot\" coords=\"6,284.65,201.96,3.99,6.91\" target=\"#foot_2\">3</ref>As the LASLA corpus includes a diverse range of classical Latin texts, it represents an interesting test set to investigate the generalisability of the models.</p><p xml:id=\"_CbdXCuj\">With this procedure, we also establish criteria for the annotation of the most problematic classes. In addition, we augment the test set by including both prose and poetry works (resp. in-domain and outof-domain) which do not appear in the training data and that belong to different genres with respect to the training data. Overall, this process allows us to reach conclusions on the urgency of guidelines, of data generation, and the generalisability of existing models across different projects.</p><p xml:id=\"_PpmsEnm\">The portion of the LASLA corpus used for this experiment is composed of 1,738,435 tokens, belonging to 130 Latin literary texts by 21 authors ranging from the 2nd century BCE to the 2nd century CE. It is linked to the LiLa Knowledge</p><p xml:id=\"_Ff8Ca5s\">Base, an open-ended Knowledge Base of linguistic Linked Data <ref type=\"bibr\" coords=\"6,130.55,475.55,103.28,9.46\" target=\"#b20\">(Passarotti et al., 2020)</ref>. The URIs for lemmas and tokens provided by the linking are published to ensure interoperability and reusability of the data.<ref type=\"foot\" coords=\"6,119.64,514.15,3.99,6.91\" target=\"#foot_3\">foot_3</ref> </p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.1\" xml:id=\"_T6k47XX\">Texts annotated</head><p xml:id=\"_d9783d8\">To evaluate the performance of the models on the LASLA corpus, we annotated texts from three different authors. As in-domain data, we chose to annotate Tacitus\\' Historiae (Hist.) book 1 and the first of Cicero\\'s Orationes Philippicae (Phil.) and for out-of-domain the first three of Juvenal\\'s Saturae (Juv.). Tacitus and Cicero were selected as \\'in-domain\\' data since they belong to non-fictional prose. Moreover, the Phil. are a different genre (oratory) than the Herodotos training data and Tacitus (Historiography and Epistolography). Juvenal\\'s poetry, with its mentions of historical people, was selected to challenge the model, since the out-of-domain testing of Ovid\\'s Ars am., on the contrary, primarily mentions mythical persons. Good performance on these texts would indicate the models\\' generalisability.</p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.2\" xml:id=\"_2T7DHU2\">Annotation process and choices</head><p xml:id=\"_9S8E8Uy\">The texts were annotated by two Latin experts using the BIO-format for the entities location, person, and group (see Section 3.1). The Herodotos project annotation was taken as a reference, and the challenging points were discussed in order to address the shortcomings identified in Section 4.1. Cohesion between the annotations of the two experts was guaranteed by joint annotation of 4,463 tokens of the Saturae (Juv. 1-3). The Inter-Annotator Agreement (IAA) was calculated using Cohen\\'s Kappa score <ref type=\"bibr\" coords=\"6,332.50,330.35,62.37,9.46\" target=\"#b4\">(Cohen, 1960)</ref>. The IAA is calculated both including and excluding the label \\'O\\'. The resulting values are 0.87 (incl. \\'O\\') and 0.74 (excl. \\'O\\'). The confusion matrix (excl. \\'O\\') is shown in Figure <ref type=\"figure\" coords=\"6,322.33,384.54,5.35,9.46\" target=\"#fig_0\">1</ref> of the Appendix A. The biggest disagreement concerns the label \\'B-GRP\\'. The difficulties with the annotation of \\'GRP\\' can be divided into two categories: annotation of adjectives derived from toponyms (Tuscus -\\'Tuscan\\', Aegyptius -\\'Egyptian\\', Graecus -\\'Greek\\') and groups of individuals that do not fit the definition of political/ethnic groups as defined by the Herodotos project. Examples of this last category are names of families (e.g Gracchos (2.24) -\\'The Gracchii\\'), names used as a generic category (e.g. Proculas et Pollittas (2.68) -\\'women like Procula and Pollitta\\'), gods (Asianorum ... deorum (3.218) -\\'Asian gods\\'), and other groups such as Socraticos ... cinaedos (2.10 -\\'Socratic catamites\\') and Manes (2.149 -\\'Shades\\'). For adjectives derived from toponyms, the annotators agreed to use \\'GRP\\' to align with the Herodotos project. For the other categories, \\'GRP\\' is used following the definition of the subcategory \\'PER.Group\\' from the Automatic Content Extraction Guidelines <ref type=\"bibr\" coords=\"6,404.61,655.53,86.32,9.46\">(Consortium, 2008)</ref> for any Person entity referring to more than one person. Finally, we chose not to annotate nicknames as \\'PERS\\' entities (e.g. Uenusina ... lucerna (1.51) -\\'The Venusinian light\\', Horace, was only annotated as \\'B-LOC ... O\\'). Following the first round of joint annotation, an agreement was reached on problematic points to enhance the consistency of the remaining annotation. </p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.3\" xml:id=\"_wfXs4W6\">Results of running the model</head><p xml:id=\"_zBfrKNP\">Table <ref type=\"table\" coords=\"7,98.67,403.48,5.56,9.46\">6</ref> shows that when labelling single tokens LatinBERT2 outperforms the other models on in-domain data, whereas the models score very close on out-of-domain data, with LatinCY scoring slightly higher than LatinBERT2. 5 Table <ref type=\"table\" coords=\"7,253.99,457.68,5.45,9.46\" target=\"#tab_6\">7</ref> shows that LatinBERT2 predicts entire entities better than the other models, except for the category \\'LOC\\' on out-of-domain data, where LatinBERT1 performs better. These results confirm LatinBERT2\\'s general good performance, but also its again somewhat unexpected behavior on poetry.</p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.4\" xml:id=\"_4WEascM\">Error Analysis</head></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.4.1\" xml:id=\"_4dTqNuF\">Challenging aspects of NER prediction</head><p xml:id=\"_qRuuNZD\">Similarly as to the Herodotos data, many errors can again be related to the inherent ambiguity of Latin and/or the choices made in annotation (cf. Section 4.1). Both on the in-and out-of-domain LASLA data, errors were made that are related 5 The major increase in performance of LatinCy on the LASLA data can be explained by two reasons: first, 38% of total errors of LatinCy concern the GRP-entities, of which there are relatively less in the LASLA test data (23.5% of the total entities are \\'GRP\\'s in Herodotos, whereas in the LASLA 14.1%); second, many other errors are caused by the tendency of LatinCy to predict entities for any and all capitalized words. In the Herodotos data, all sentences start with a capital, creating many errors for LatinCy; in the LASLA, capitalization is absent, hence such errors do not occur.</p><p xml:id=\"_6JAeQYs\">to ambiguous tokens that occur both as entity and non-entity, albeit slightly more present in out-ofdomain, e.g. Pax atque Fides, Uictoria, Uirtus (\\'The Goddesses Peace, Faith, Victory and Virtue <ref type=\"bibr\" coords=\"7,517.48,425.47,8.29,9.46\">\\',</ref><ref type=\"bibr\" coords=\"7,305.94,439.02,48.55,9.46\">Juv. 1.115)</ref>. Also for the LASLA test-set, tokens annotated differently across the Herodotos training data result in multiple errors. For instance, non-capitalized forms of prouincia and urbs are annotated as \\'LOC\\' in the training data only when they refer to a precise location. Likewise, princeps and imperator are annotated as \\'PERS\\' only where they refer to specific emperors. Lastly, words like domus and aedes are sometimes annotated when they indicate a specific location: for example, aede Apollinis -\\'the temple of Apollo\\' and Tiberianam domum -\\'the palace of Tiberius\\'. Even though the Herodotos training data are not fully consistent in these annotations, the LASLA annotation did strictly follow these guidelines, which highlighted the inconsistent behavior of models with respect to these points. A close analysis of the performance on tokens where the manual annotation differed shows some additional challenging categories. Of the 69 tokens where the manual annotation differed, LatinBERT1 got 39 wrong (accounting for 20.5% of its total errors), and LatinBERT2 got 41 wrong (accounting for 22.5% of its total errors). For instance, both LatinBERTs predict \\'O\\' for most groups of individuals that did not fit the political/ethnical \\'GRP\\' category, except for some family names (e.g. Catuli, Fabii). For Literary works identified by a personal name, another category where the annotators disagreed but were eventually not annotated, LatinBERT2 predicts an entity but LatinBERT1 \\'O\\' (e.g. Theseide (1.2); Heracleas | aut Diomedeas (1.52-3)). Lastly, for the category of persons referred to with only a toponym, also identified as an issue in Section 4.1, we annotated \\'LOC\\' but the LatinBERTs predicted \\'GRP\\': e.g. non Maurus erat neque Sarmata nec Thrax (\\'it was not a Moroccan nor a Sarmatian nor a Thracian\\', 3.79).</p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"5.4.2\" xml:id=\"_bTGcXGD\">Qualitative analysis</head><p xml:id=\"_n2W697X\">The comparison between the two LatinBERTs shows that on the in-domain LASLA data, Latin-BERT2 outperforms LatinBERT1, especially on I-labels (cf. Appendix A, Table <ref type=\"table\" coords=\"8,213.23,657.34,4.02,9.46\" target=\"#tab_9\">9</ref>). When considering I-label errors, both LatinBERTs classify the category correctly for more than half of these errors (40 out of 78 for LatinBERT1; 32 out of 62 for Lat-inBERT2), but wrongly assign the \\'B-\\' label: the problem thus lies again with the boundary detec-tion. On the out-of-domain data, LatinBERT2 outperforms LatinBERT1 in the \\'B-PERS\\' category. As on the Herodotos project test data, in the majority cases where only LatinBERT1 is correct, Latin-BERT2 predicts a non-entity: for the in-domain set 22 out of 27 total cases concern words absent from the train/validation set, for out-of-domain 16 out of 18.</p><p xml:id=\"_qhnvTeN\">This analysis confirmed that the categories identified in Section 4.2 are difficult for NER. It also emphasised the differences between in-and out-ofdomain data: models only trained on prose perform worse on poetry due to stylistic and thematic differences.</p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head n=\"6\" xml:id=\"_dX5PtFm\">Conclusions and future work</head><p xml:id=\"_KwTsuAc\">The process of training two new models on existing data, comparing their results on previously and newly annotated data, and comparing their performance to existing models allows us to draw several conclusions. First, the good performance of LatinBERT1 and 2 demonstrates the interest of applying transformer-based models for the NER task on Latin. Especially for the category \\'PERS\\' the two models yield satisfactory results. However, the analysis of the annotations and the errors has shown that the development of guidelines is crucial to ensure the consistent annotation of datasets that can be reused as training-and test-sets across different projects and for different models. In addition, the significantly worse performance of the models on poetry indicates the need for training data for this specific type of texts. Future work should also consider improving the preprocessing and normalization of training data (e.g. harmonizing the use of the \\'v/u\\' \\'i/j\\' pairs), and testing the use of multilingual BERT models that include Latin (mBERT, XLM-Roberta) <ref type=\"bibr\" coords=\"8,421.45,614.88,103.86,9.46\" target=\"#b24\">(Sprugnoli et al., 2022;</ref><ref type=\"bibr\" coords=\"8,306.14,628.43,71.61,9.46\" target=\"#b17\">Nehrdich, 2022)</ref>. Likewise, additional linguistic information available in the LASLA corpus (e.g. lemmatization and PoS tagging) might improve the results of the NER. Finally, after we establish a system for Named Entity Disambiguation employing information from existing extensive resources, we will explore the potential of mutual reinforcement, i.e. we will consider whether results from one system can improve the other and vice-versa as argued by <ref type=\"bibr\" coords=\"8,319.78,750.37,89.31,9.46\" target=\"#b14\">Kolitsas et al. (2018)</ref>.  </p></div>\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\"><head xml:id=\"_7vgR2Ga\">A Appendix</head></div><figure xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"fig_0\" coords=\"11,71.83,278.33,216.35,8.81;11,82.98,103.57,194.04,163.08\"><head>Figure 1 :</head><label>1</label><figDesc xml:id=\"_kgePTjW\">Figure 1: IAA on Juv. Saturae 1-3, label \\'O\\' excluded</figDesc><graphic coords=\"11,82.98,103.57,194.04,163.08\" type=\"bitmap\" /></figure>\\n<figure xmlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_0\" coords=\"2,249.76,76.89,274.65,696.49\"><head></head><label></label><figDesc xml:id=\"_dmrD7w8\">, is avail-</figDesc><table coords=\"2,305.83,76.89,218.58,182.47\"><row><cell>text</cell><cell># tokens</cell></row><row><cell>BGall.</cell><cell>58,621</cell></row><row><cell>NH</cell><cell>35,672</cell></row><row><cell>Ep.</cell><cell>18,571</cell></row><row><cell>Ars am.</cell><cell>17,102</cell></row><row><cell>BCiv.</cell><cell>4,819</cell></row><row><cell cols=\"2\">Table 1: Number of tokens per text in the Herodotos</cell></row><row><cell>dataset</cell><cell></cell></row><row><cell cols=\"2\">able online. 2 We decided to annotate new material</cell></row><row><cell cols=\"2\">to augment the availability of data for classical</cell></row><row><cell>Latin.</cell><cell></cell></row></table></figure>\\n<figure xmlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_1\" coords=\"2,306.14,720.45,218.27,23.01\"><head></head><label></label><figDesc xml:id=\"_dH3MDKP\">The out-domain part consisted of the one poetry text, Ars. Am..</figDesc><table coords=\"3,70.56,76.89,218.57,160.96\"><row><cell>type</cell><cell cols=\"2\">frequency</cell></row><row><cell></cell><cell cols=\"2\">Train Validation</cell></row><row><cell>O</cell><cell>82,696</cell><cell>13,846</cell></row><row><cell>B-PERS</cell><cell>2,706</cell><cell>473</cell></row><row><cell>I-PERS</cell><cell>618</cell><cell>125</cell></row><row><cell>B-LOC</cell><cell>839</cell><cell>169</cell></row><row><cell>I-LOC</cell><cell>31</cell><cell>10</cell></row><row><cell>B-GRP</cell><cell>1,271</cell><cell>207</cell></row><row><cell>I-GRP</cell><cell>4</cell><cell>2</cell></row><row><cell cols=\"3\">Table 2: Frequency of entity types in train (left) and</cell></row><row><cell>validation set (right)</cell><cell></cell><cell></cell></row></table></figure>\\n<figure xmlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_2\" coords=\"4,70.56,149.51,220.23,20.59\"><head>Table 3</head><label>3</label><figDesc xml:id=\"_ZtyCxmn\"></figDesc><table coords=\"4,70.87,149.51,219.92,20.59\"><row><cell>: initial hyperparameters (LatinBERT1) vs. opti-</cell></row><row><cell>mised hyperparameters (LatinBERT2)</cell></row></table></figure>\\n<figure xmlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_3\" coords=\"4,319.16,557.35,207.15,216.04\"><head>Table 4 :</head><label>4</label><figDesc xml:id=\"_4cZ3SsQ\">micro f1 on the Herodotos selected test-set; LB stands for LatinBERT</figDesc><table coords=\"4,499.29,557.35,26.94,9.46\"><row><cell>(\\'Am-</cell></row></table></figure>\\n<figure xmlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_4\" coords=\"5,165.33,350.96,264.31,8.64\"><head>Table 5 :</head><label>5</label><figDesc xml:id=\"_q43T6Bx\"></figDesc><table /><note><p xml:id=\"_sCXN3mj\">f1-score per entity type on the Herodotos selected test-set</p></note></figure>\\n<figure xmlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_6\" coords=\"7,91.20,76.89,412.89,282.71\"><head>Table 7 :</head><label>7</label><figDesc xml:id=\"_QApkFym\">f1-score per entity type &amp; macro f1 on the LASLA corpus</figDesc><table coords=\"7,91.20,76.89,412.89,257.69\"><row><cell>micro f1</cell><cell></cell><cell cols=\"4\">CRF LB1 LB2 LatinCy lg Herodotos support</cell></row><row><cell cols=\"3\">Tac. and Cic. (IN) BIO-labels 0.96 0.97 0.98</cell><cell>0.96</cell><cell>0.97</cell><cell>15,737</cell></row><row><cell></cell><cell>BI-labels</cell><cell>0.61 0.78 0.79</cell><cell>0.66</cell><cell>0.72</cell><cell>1,320</cell></row><row><cell>Juv. (OUT)</cell><cell cols=\"2\">BIO-labels 0.96 0.96 0.96</cell><cell>0.96</cell><cell>0.96</cell><cell>4,399</cell></row><row><cell></cell><cell>BI-labels</cell><cell>0.45 0.48 0.50</cell><cell>0.51</cell><cell>0.48</cell><cell>284</cell></row><row><cell cols=\"4\">Table 6: micro f1 on the LASLA corpus; LB stands for LatinBERT</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols=\"4\">CRF LB1 LB2 LatinCy lg Herodotos support</cell></row><row><cell cols=\"2\">Tac. and Cic. (IN) PERS</cell><cell>0.65 0.83 0.85</cell><cell>0.66</cell><cell>0.74</cell><cell>711</cell></row><row><cell></cell><cell>LOC</cell><cell>0.31 0.51 0.55</cell><cell>0.53</cell><cell>0.49</cell><cell>222</cell></row><row><cell></cell><cell>GRP</cell><cell>0.43 0.61 0.64</cell><cell>0.02</cell><cell>0.60</cell><cell>154</cell></row><row><cell></cell><cell cols=\"2\">macro f1 0.46 0.65 0.68</cell><cell>0.40</cell><cell>0.61</cell><cell>1,087</cell></row><row><cell>Juvenal (OUT)</cell><cell>PERS</cell><cell>0.48 0.53 0.64</cell><cell>0.64</cell><cell>0.59</cell><cell>143</cell></row><row><cell></cell><cell>LOC</cell><cell>0.32 0.46 0.36</cell><cell>0.44</cell><cell>0.27</cell><cell>83</cell></row><row><cell></cell><cell>GRP</cell><cell>0.47 0.40 0.52</cell><cell>0.00</cell><cell>0.23</cell><cell>36</cell></row><row><cell></cell><cell cols=\"2\">macro f1 0.43 0.46 0.51</cell><cell>0.36</cell><cell>0.37</cell><cell>262</cell></row></table></figure>\\n<figure xmlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_8\" coords=\"11,70.56,464.57,239.79,207.14\"><head>Table 8 :</head><label>8</label><figDesc xml:id=\"_GUBZ7pS\">Comparison of differences in prediction between LatinBERT1 (1) and LatinBERT2 (2) on the Herodotos data.</figDesc><table coords=\"11,76.84,528.48,233.50,143.23\"><row><cell cols=\"5\">Gold label 1 &amp; 2 wrong 1 correct</cell><cell cols=\"2\">2 correct</cell></row><row><cell></cell><cell>IN</cell><cell cols=\"5\">OUT IN OUT IN OUT</cell></row><row><cell>O</cell><cell>0</cell><cell>0</cell><cell>7</cell><cell>5</cell><cell>7</cell><cell>7</cell></row><row><cell>B-PERS</cell><cell>1</cell><cell cols=\"2\">6 14</cell><cell cols=\"2\">8 20</cell><cell>22</cell></row><row><cell>I-PERS</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell cols=\"2\">0 14</cell><cell>0</cell></row><row><cell>B-LOC</cell><cell>2</cell><cell>7</cell><cell>6</cell><cell cols=\"2\">15 11</cell><cell>9</cell></row><row><cell>I-LOC</cell><cell>3</cell><cell>3</cell><cell>0</cell><cell>0</cell><cell>5</cell><cell>0</cell></row><row><cell>B-GRP</cell><cell>14</cell><cell>3</cell><cell>6</cell><cell cols=\"2\">4 10</cell><cell>5</cell></row><row><cell>I-GRP</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell></cell></row><row><cell>Total</cell><cell>23</cell><cell cols=\"2\">22 37</cell><cell cols=\"2\">32 67</cell><cell>43</cell></row></table></figure>\\n<figure xmlns=\"http://www.tei-c.org/ns/1.0\" type=\"table\" xml:id=\"tab_9\" coords=\"11,70.56,688.10,220.23,32.55\"><head>Table 9 :</head><label>9</label><figDesc xml:id=\"_NFnjz3X\">Comparison of differences in prediction between LatinBERT1 (1) and LatinBERT2 (2) on in and out-of-domain LASLA data.</figDesc><table /></figure>\\n\\t\\t\\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"1\" xml:id=\"foot_0\"><p xml:id=\"_autn7s4\">https://github.com/NER-AncientLanguages/ Ner-Latin-RANLP.</p></note>\\n\\t\\t\\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"2\" xml:id=\"foot_1\"><p xml:id=\"_vTrH6Xf\">https://gitlab.com/magistermilitum/ner_ medieval_multilingual/</p></note>\\n\\t\\t\\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"3\" xml:id=\"foot_2\"><p xml:id=\"_H9sdeqk\">https://www.lasla.uliege.be</p></note>\\n\\t\\t\\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"4\" xml:id=\"foot_3\"><p xml:id=\"_G5fguP2\">https://github.com/NER-AncientLanguages/ Ner-Latin-RANLP</p></note>\\n\\t\\t\\t<note xmlns=\"http://www.tei-c.org/ns/1.0\" place=\"foot\" n=\"6\" xml:id=\"foot_4\"><p xml:id=\"_QAPw9sg\">This is particularly surprising since in the Herodotos testset LatinBERT1 correctly predicted 29 out of 40 of such forms, and LatinBERT2 22.</p></note>\\n\\t\\t</body>\\n\\t\\t<back>\\n\\t\\t\\t<div type=\"annex\">\\n<div xmlns=\"http://www.tei-c.org/ns/1.0\" xml:id=\"_C8PYBH2\" />\\t\\t\\t</div>\\n\\t\\t\\t<div type=\"references\">\\n\\n\\t\\t\\t\\t<listBibl>\\n\\n<biblStruct coords=\"9,70.87,92.60,219.65,8.64;9,81.42,103.55,209.46,8.64;9,81.45,114.51,180.45,8.64\" xml:id=\"b0\">\\n\\t<monogr>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">David</forename><surname>Bamman</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Patrick</forename><forename type=\"middle\">J</forename><surname>Burns</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno type=\"DOI\">10.48550/arXiv.2009.10053</idno>\\n\\t\\t<idno type=\"arXiv\">arXiv:2009.10053</idno>\\n\\t\\t<idno>ArXiv:2009.10053</idno>\\n\\t\\t<title level=\"m\" xml:id=\"_RDZ8eHK\">Latin bert: A contextual language model for classical philology</title>\\n\\t\\t<imprint>\\n\\t\\t\\t<date type=\"published\" when=\"2020\">2020</date>\\n\\t\\t</imprint>\\n\\t</monogr>\\n\\t<note>cs</note>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,70.87,133.49,218.27,8.64;9,81.78,144.45,105.39,8.64;9,207.59,144.45,83.29,8.64;9,81.42,155.41,95.21,8.64\" xml:id=\"b1\">\\n\\t<monogr>\\n\\t\\t<title level=\"m\" type=\"main\" xml:id=\"_xujFTFb\">Latincy: Synthetic trained pipelines for latin nlp</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">J</forename><surname>Patrick</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><surname>Burns</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno type=\"arXiv\">arXiv:2305.04365</idno>\\n\\t\\t<idno>ArXiv:2305.04365</idno>\\n\\t\\t<imprint>\\n\\t\\t\\t<date type=\"published\" when=\"2023\">2023</date>\\n\\t\\t</imprint>\\n\\t</monogr>\\n\\t<note>cs</note>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,70.87,174.38,218.62,8.64;9,81.03,185.34,39.02,8.64\" xml:id=\"b2\">\\n\\t<monogr>\\n\\t\\t<title/>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">William</forename><forename type=\"middle\">L</forename><surname>Carey</surname></persName>\\n\\t\\t</author>\\n\\t\\t<imprint>\\n\\t\\t\\t<date type=\"published\" when=\"2023-07-01\">July 1st, 2023</date>\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,70.87,204.32,219.92,8.64;9,81.78,215.27,207.52,8.64;9,81.78,226.06,209.01,8.81;9,81.78,237.02,52.29,8.81\" xml:id=\"b3\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_Mh8wzY5\">A Named Entity Recognition Model for Medieval Latin Charters</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Pierre</forename><surname>Chastang</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Sergio</forename><forename type=\"middle\">Torres</forename><surname>Aguilar</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Xavier</forename><surname>Tannier</surname></persName>\\n\\t\\t</author>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"j\" xml:id=\"_UpXq8Dc\">Digital Humanities Quarterly</title>\\n\\t\\t<imprint>\\n\\t\\t\\t<biblScope unit=\"issue\">4</biblScope>\\n\\t\\t\\t<biblScope unit=\"page\">15</biblScope>\\n\\t\\t\\t<date type=\"published\" when=\"2021\">2021</date>\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,70.87,256.17,218.44,8.64;9,81.78,266.96,207.35,8.81;9,81.36,277.92,110.58,8.81\" xml:id=\"b4\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_JqtWYbr\">A Coefficient of Agreement for Nominal Scales</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Jacob</forename><surname>Cohen</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno type=\"DOI\">10.1177/001316446002000104</idno>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"j\" xml:id=\"_9Rvx8HB\">Educational and Psychological Measurement</title>\\n\\t\\t<imprint>\\n\\t\\t\\t<biblScope unit=\"volume\">20</biblScope>\\n\\t\\t\\t<biblScope unit=\"issue\">1</biblScope>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"37\" to=\"46\" />\\n\\t\\t\\t<date type=\"published\" when=\"1960\">1960</date>\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,70.87,297.06,218.27,8.64;9,81.78,308.02,207.36,8.64;9,81.78,318.98,47.04,8.64\" xml:id=\"b5\">\\n\\t<monogr>\\n\\t\\t<title level=\"m\" type=\"main\" xml:id=\"_yAw5A3C\">ACE (Automatic Content Extraction) English: Annotation Guidelines for Entities</title>\\n\\t\\t<imprint>\\n\\t\\t\\t<date type=\"published\" when=\"2008\">2008</date>\\n\\t\\t</imprint>\\n\\t</monogr>\\n\\t<note>Linguistic Data Consortium</note>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,70.87,337.95,218.27,8.64;9,81.78,348.91,207.36,8.64;9,81.78,359.87,209.02,8.64;9,81.78,370.66,134.48,8.81\" xml:id=\"b6\">\\n\\t<monogr>\\n\\t\\t<title level=\"m\" type=\"main\" xml:id=\"_Kkkzjvv\">BERT: pre-training of deep bidirectional transformers for language understanding</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno>CoRR, abs/1810.04805</idno>\\n\\t\\t<imprint>\\n\\t\\t\\t<date type=\"published\" when=\"2018\">2018</date>\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,70.87,389.80,219.51,8.64;9,81.78,400.76,209.10,8.64;9,81.78,411.72,209.01,8.64;9,81.78,422.68,209.10,8.64;9,81.42,433.64,95.21,8.64\" xml:id=\"b7\">\\n\\t<monogr>\\n\\t\\t<title level=\"m\" type=\"main\" xml:id=\"_797erum\">Named entity recognition and classification on historical documents: A survey</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Maud</forename><surname>Ehrmann</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Ahmed</forename><surname>Hamdi</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Linhares</forename><surname>Elvys</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Matteo</forename><surname>Pontes</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Antoine</forename><surname>Romanello</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><surname>Doucet</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno type=\"arXiv\">arXiv:2109.11406</idno>\\n\\t\\t<idno>ArXiv:2109.11406</idno>\\n\\t\\t<imprint>\\n\\t\\t\\t<date type=\"published\" when=\"2021\">2021</date>\\n\\t\\t</imprint>\\n\\t</monogr>\\n\\t<note>cs</note>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,70.87,452.62,219.92,8.64;9,81.78,463.57,209.10,8.64;9,81.78,474.53,207.36,8.64;9,81.78,485.49,207.36,8.64;9,81.47,496.28,209.32,8.58;9,81.78,507.24,209.01,8.81;9,81.78,518.37,207.36,8.64;9,81.78,529.33,45.12,8.64\" xml:id=\"b8\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_czhqbq6\">Overview of hipe-2022: Named entity recognition and linking in multilingual historical documents</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Maud</forename><surname>Ehrmann</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Matteo</forename><surname>Romanello</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Sven</forename><surname>Najem-Meyer</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Antoine</forename><surname>Doucet</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Simon</forename><surname>Clematide</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno type=\"DOI\">10.1007/978-3-031-13643-6_26</idno>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_vCNJHsG\">Experimental IR Meets Multilinguality, Multimodality, and Interaction</title>\\n\\t\\t<title level=\"s\" xml:id=\"_wnycpGf\">Lecture Notes in Computer Science</title>\\n\\t\\t<meeting><address><addrLine>Cham</addrLine></address></meeting>\\n\\t\\t<imprint>\\n\\t\\t\\t<publisher>Springer International Publishing</publisher>\\n\\t\\t\\t<date type=\"published\" when=\"2022\">2022</date>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"423\" to=\"446\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,70.87,548.30,219.51,8.64;9,81.78,559.26,209.01,8.64;9,81.78,570.22,209.01,8.64;9,81.78,581.01,209.01,8.81;9,81.78,591.97,209.01,8.58;9,81.78,602.93,208.60,8.58;9,81.78,614.06,209.01,8.64;9,81.78,625.02,83.53,8.64\" xml:id=\"b9\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_ZYy9NEF\">Challenges and solutions for Latin named entity recognition</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Alexander</forename><surname>Erdmann</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Christopher</forename><surname>Brown</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Brian</forename><surname>Joseph</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Mark</forename><surname>Janse</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Petra</forename><surname>Ajaka</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Micha</forename><surname>Elsner</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Marie-Catherine</forename><surname>De Marneffe</surname></persName>\\n\\t\\t</author>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_6v3FgKX\">Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities</title>\\n\\t\\t<meeting>the Workshop on Language Technology Resources and Tools for Digital Humanities<address><addrLine>Osaka, Japan</addrLine></address></meeting>\\n\\t\\t<imprint>\\n\\t\\t\\t<date type=\"published\" when=\"2016\">2016</date>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"85\" to=\"93\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n\\t<note>LT4DH. The COLING 2016 Organizing Committee</note>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,70.87,643.99,218.26,8.64;9,81.42,654.95,208.96,8.64;9,81.78,665.91,207.36,8.64;9,81.59,676.87,209.29,8.64;9,81.78,687.83,207.36,8.64;9,81.78,698.78,207.36,8.64;9,81.78,709.57,207.36,8.81;9,81.78,720.53,207.35,8.58;9,81.45,731.49,209.34,8.58;9,81.78,742.45,207.36,8.81;9,81.78,753.58,207.53,8.64;9,81.78,764.54,108.49,8.64\" xml:id=\"b10\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_q74ahQn\">Practical, efficient, and customizable active learning for named entity recognition in the digital humanities</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Alexander</forename><surname>Erdmann</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">David</forename><surname>Joseph Wrisley</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Benjamin</forename><surname>Allen</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Christopher</forename><surname>Brown</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Sophie</forename><surname>Cohen-Bod\\xc3\\xa9n\\xc3\\xa8s</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Micha</forename><surname>Elsner</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Yukun</forename><surname>Feng</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Brian</forename><surname>Joseph</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">B\\xc3\\xa9atrice</forename><surname>Joyeux-Prunel</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Marie-Catherine</forename><surname>De Marneffe</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno type=\"DOI\">10.18653/v1/N19-1231</idno>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_TXyeDeW\">Proceedings of the 2019 Conference of the North American Chapter</title>\\n\\t\\t<title level=\"s\" xml:id=\"_pry45Ve\">Long and Short Papers</title>\\n\\t\\t<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>\\n\\t\\t<imprint>\\n\\t\\t\\t<publisher>Association for Computational Linguistics</publisher>\\n\\t\\t\\t<date type=\"published\" when=\"2019\">2019</date>\\n\\t\\t\\t<biblScope unit=\"volume\">1</biblScope>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"2223\" to=\"2234\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,306.14,75.34,218.26,8.64;9,316.69,86.30,208.96,8.64;9,317.05,97.26,209.01,8.64;9,317.05,108.22,209.01,8.64;9,317.05,119.18,47.83,8.64;9,382.55,119.18,143.51,8.64;9,317.05,130.13,44.55,8.64\" xml:id=\"b11\">\\n\\t<monogr>\\n\\t\\t<title/>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Alexander</forename><surname>Erdmann</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">David</forename><surname>Joseph Wrisley</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Benjamin</forename><surname>Allen</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Christopher</forename><surname>Brown</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Sophie</forename><surname>Cohen-Bod\\xc3\\xa9n\\xc3\\xa8s</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Micha</forename><surname>Elsner</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Yukun</forename><surname>Feng</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Brian</forename><surname>Joseph</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">B\\xc3\\xa9atrice</forename><surname>Joyeux-Prunel</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Marie-Catherine</forename><surname>De Marn</surname></persName>\\n\\t\\t</author>\\n\\t\\t<ptr target=\"effe.2023.Herodotos-project-latin-ner-tagger-annotation\" />\\n\\t\\t<imprint/>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,306.14,148.64,219.65,8.64;9,317.05,159.60,209.01,8.64;9,317.05,170.56,207.35,8.64;9,317.05,181.52,76.66,8.64\" xml:id=\"b12\">\\n\\t<monogr>\\n\\t\\t<title level=\"m\" type=\"main\" xml:id=\"_M7Ja4zD\">2017. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Matthew</forename><surname>Honnibal</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Ines</forename><surname>Montani</surname></persName>\\n\\t\\t</author>\\n\\t\\t<imprint/>\\n\\t</monogr>\\n\\t<note>To appear</note>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,306.14,200.03,218.27,8.64;9,317.05,210.99,209.10,8.64;9,317.05,221.95,209.01,8.64;9,316.69,232.74,207.72,8.81;9,317.05,243.70,207.36,8.58;9,316.72,254.65,207.69,8.58;9,316.61,265.61,209.45,8.58;9,316.80,276.57,209.26,8.81;9,317.05,287.70,145.02,8.64\" xml:id=\"b13\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_dKHRuBH\">The classical language toolkit: An nlp framework for pre-modern languages</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Kyle</forename><forename type=\"middle\">P</forename><surname>Johnson</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Patrick</forename><forename type=\"middle\">J</forename><surname>Burns</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">John</forename><surname>Stewart</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Todd</forename><surname>Cook</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Cl\\xc3\\xa9ment</forename><surname>Besnier</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">William</forename><forename type=\"middle\">J B</forename><surname>Mattingly</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno type=\"DOI\">10.18653/v1/2021.acl-demo.3</idno>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_xWtKwYp\">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</title>\\n\\t\\t<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</meeting>\\n\\t\\t<imprint>\\n\\t\\t\\t<date type=\"published\" when=\"2021\">2021</date>\\n\\t\\t</imprint>\\n\\t</monogr>\\n\\t<note>page 20-29, Online. Association for Computational Linguistics</note>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,306.14,306.21,218.26,8.64;9,317.05,317.17,209.10,8.64;9,317.05,327.96,207.36,8.81;9,317.05,339.09,209.10,8.64\" xml:id=\"b14\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_3VFDVhj\">End-to-End Neural Entity Linking</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Nikolaos</forename><surname>Kolitsas</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><surname>Octavian-Eugen</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Thomas</forename><surname>Ganea</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><surname>Hofmann</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno type=\"DOI\">10.18653/v1/K18-1050</idno>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_mQVbXxa\">Computational Natural Language Learning</title>\\n\\t\\t<imprint>\\n\\t\\t\\t<publisher>Association for Computational Linguistics</publisher>\\n\\t\\t\\t<date type=\"published\" when=\"2018\">2018</date>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"519\" to=\"529\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,306.14,357.59,219.92,8.64;9,317.05,368.55,209.10,8.64;9,317.05,379.51,209.10,8.64;9,317.05,390.30,207.36,8.81;9,316.44,401.26,209.62,8.58;9,317.05,412.22,208.60,8.58;9,317.05,423.35,207.36,8.64;9,317.05,434.31,122.60,8.64\" xml:id=\"b15\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_8ZqdV68\">Neural architectures for named entity recognition</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Guillaume</forename><surname>Lample</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Miguel</forename><surname>Ballesteros</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Sandeep</forename><surname>Subramanian</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Kazuya</forename><surname>Kawakami</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Chris</forename><surname>Dyer</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno type=\"DOI\">10.18653/v1/N16-1030</idno>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_QADntHH\">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>\\n\\t\\t<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>\\n\\t\\t<imprint>\\n\\t\\t\\t<publisher>Association for Computational Linguistics</publisher>\\n\\t\\t\\t<date type=\"published\" when=\"2016\">2016</date>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"260\" to=\"270\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,306.14,452.82,218.27,8.64;9,317.05,463.77,207.36,8.64;9,316.74,474.56,207.67,8.58;9,316.49,485.52,209.16,8.58;9,317.05,496.65,209.01,8.64;9,317.05,507.61,119.25,8.64\" xml:id=\"b16\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_wAUsKzs\">An ELECTRA model for Latin token tagging tasks</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Wouter</forename><surname>Mercelis</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Alek</forename><surname>Keersmaekers</surname></persName>\\n\\t\\t</author>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_fKgSRE7\">Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages</title>\\n\\t\\t<meeting>the Second Workshop on Language Technologies for Historical and Ancient Languages<address><addrLine>Marseille, France</addrLine></address></meeting>\\n\\t\\t<imprint>\\n\\t\\t\\t<publisher>European Language Resources Association</publisher>\\n\\t\\t\\t<date type=\"published\" when=\"2022\">2022</date>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"189\" to=\"192\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,306.14,526.12,219.92,8.64;9,317.05,537.08,209.01,8.64;9,317.05,547.87,209.01,8.81;9,317.05,558.83,207.36,8.81;9,317.05,569.95,207.36,8.64;9,317.05,580.91,92.97,8.64\" xml:id=\"b17\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_AaAuwFa\">SansTib, a Sanskrit -Tibetan parallel corpus and bilingual sentence embedding model</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Sebastian</forename><surname>Nehrdich</surname></persName>\\n\\t\\t</author>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_KmQtS7c\">Proceedings of the Thirteenth Language Resources and Evaluation Conference</title>\\n\\t\\t<meeting>the Thirteenth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>\\n\\t\\t<imprint>\\n\\t\\t\\t<publisher>European Language Resources Association</publisher>\\n\\t\\t\\t<date type=\"published\" when=\"2022\">2022</date>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"6728\" to=\"6734\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,306.14,599.42,218.27,8.64;9,317.05,610.38,139.18,8.64\" xml:id=\"b18\">\\n\\t<monogr>\\n\\t\\t<title level=\"m\" type=\"main\" xml:id=\"_RYKa5E6\">Crfsuite: a fast implementation of conditional random fields (crfs)</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Naoaki</forename><surname>Okazaki</surname></persName>\\n\\t\\t</author>\\n\\t\\t<imprint>\\n\\t\\t\\t<date type=\"published\" when=\"2007\">2007</date>\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,306.14,628.89,220.01,8.64;9,317.05,639.85,209.10,8.64;9,317.05,650.64,101.76,8.58\" xml:id=\"b19\">\\n\\t<monogr>\\n\\t\\t<title level=\"m\" type=\"main\" xml:id=\"_SQGFSnN\">Ner on ancient greek with minimal annotation</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Chiara</forename><surname>Palladino</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Farimah</forename><surname>Karimi</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Brigitte</forename><surname>Mathiak</surname></persName>\\n\\t\\t</author>\\n\\t\\t<ptr target=\"https://dh2020.adho.org/\" />\\n\\t\\t<imprint>\\n\\t\\t\\t<date type=\"published\" when=\"2020\">2020</date>\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,306.14,669.32,219.51,8.64;9,317.05,680.28,209.01,8.64;9,316.80,691.23,209.36,8.64;9,317.05,702.19,209.01,8.64;9,317.05,713.15,207.35,8.64;9,317.05,723.94,202.09,8.81\" xml:id=\"b20\">\\n\\t<monogr>\\n\\t\\t<title level=\"m\" type=\"main\" xml:id=\"_v7yq5R8\">Interlinking through lemmas. the lexical collection of the lila knowledge base of linguistic resources for latin</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Marco</forename><surname>Passarotti</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Francesco</forename><surname>Mambrini</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Greta</forename><surname>Franzini</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Massimiliano</forename><surname>Flavio</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Eleonora</forename><surname>Cecchini</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Giovanni</forename><surname>Litta</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Paolo</forename><surname>Moretti</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Rachele</forename><surname>Ruffolo</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><surname>Sprugnoli</surname></persName>\\n\\t\\t</author>\\n\\t\\t<imprint>\\n\\t\\t\\t<date type=\"published\" when=\"2020\">2020</date>\\n\\t\\t\\t<biblScope unit=\"volume\">58</biblScope>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"177\" to=\"212\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n\\t<note>Studi e Saggi Linguistici</note>\\n</biblStruct>\\n\\n<biblStruct coords=\"9,306.14,742.62,220.01,8.64;9,317.05,753.58,207.36,8.64;9,317.05,764.54,90.76,8.64\" xml:id=\"b21\">\\n\\t<monogr>\\n\\t\\t<title level=\"m\" type=\"main\" xml:id=\"_qHU69yn\">Guidelines for the annotation of named entities in the domain of classics</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Matteo</forename><surname>Romanello</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Sven</forename><surname>Najem-Meyer</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno type=\"DOI\">10.5281/zenodo.6368101</idno>\\n\\t\\t<imprint>\\n\\t\\t\\t<date type=\"published\" when=\"2022\">2022</date>\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"10,70.87,75.34,220.01,8.64;10,81.78,86.30,207.36,8.64;10,81.78,97.09,191.36,8.81\" xml:id=\"b22\">\\n\\t<monogr>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">David</forename><forename type=\"middle\">A</forename><surname>Smith</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Jeffrey</forename><forename type=\"middle\">A</forename><surname>Rydberg-Cox</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">G</forename><surname>Crane</surname></persName>\\n\\t\\t</author>\\n\\t\\t<title level=\"m\" xml:id=\"_zsyTD9N\">The perseus project: a digital library for the humanities. Literary and Linguistic Computing</title>\\n\\t\\t<imprint>\\n\\t\\t\\t<date type=\"published\" when=\"2000\">2000</date>\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"10,70.87,117.65,219.92,8.64;10,81.78,128.61,208.60,8.64;10,81.59,139.57,208.80,8.64;10,81.78,150.53,207.52,8.64;10,81.78,161.32,209.02,8.81;10,81.78,172.28,79.14,8.81\" xml:id=\"b23\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_FSTB42p\">Ion Androutsopoulos, and Nando de Freitas. 2023. Machine learning for ancient languages: A survey</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Thea</forename><surname>Sommerschield</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Yannis</forename><surname>Assael</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">John</forename><surname>Pavlopoulos</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Vanessa</forename><surname>Stefanak</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Andrew</forename><surname>Senior</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Chris</forename><surname>Dyer</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">John</forename><surname>Bodel</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Jonathan</forename><surname>Prag</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno type=\"DOI\">10.1162/coli_a_00481</idno>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"j\" xml:id=\"_hvhWjbd\">Computational Linguistics</title>\\n\\t\\t<imprint>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"1\" to=\"44\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"10,70.87,192.84,219.92,8.64;10,81.78,203.80,207.36,8.64;10,81.78,214.76,209.01,8.64;10,81.78,225.55,207.36,8.81;10,80.95,236.51,208.18,8.58;10,81.78,247.47,208.61,8.81;10,81.78,258.60,207.50,8.64\" xml:id=\"b24\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_yNEJBkv\">Overview of the EvaLatin 2022 evaluation campaign</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Rachele</forename><surname>Sprugnoli</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Marco</forename><surname>Passarotti</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Massimiliano</forename><surname>Flavio</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Margherita</forename><surname>Cecchini</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Giovanni</forename><surname>Fantoli</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><surname>Moretti</surname></persName>\\n\\t\\t</author>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_hGtfMPr\">Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages</title>\\n\\t\\t<meeting>the Second Workshop on Language Technologies for Historical and Ancient Languages<address><addrLine>Marseille, France</addrLine></address></meeting>\\n\\t\\t<imprint>\\n\\t\\t\\t<publisher>European Language Resources Association</publisher>\\n\\t\\t\\t<date type=\"published\" when=\"2022\">2022</date>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"183\" to=\"188\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"10,70.87,278.99,218.62,8.64;10,81.78,289.95,209.01,8.64;10,81.78,300.74,207.36,8.81;10,81.78,311.70,207.35,8.58;10,81.42,322.66,208.96,8.81;10,81.78,333.79,207.36,8.64;10,81.42,344.75,49.53,8.64\" xml:id=\"b25\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_qxa6pPY\">Multilingual named entity recognition for medieval charters using stacked embeddings and bert-based models</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Sergio</forename><surname>Torres</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Aguilar</forename></persName>\\n\\t\\t</author>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_bCrRTCZ\">Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages</title>\\n\\t\\t<meeting>the Second Workshop on Language Technologies for Historical and Ancient Languages<address><addrLine>Marseille, France</addrLine></address></meeting>\\n\\t\\t<imprint>\\n\\t\\t\\t<publisher>European Language Resources Association</publisher>\\n\\t\\t\\t<date type=\"published\" when=\"2022\">2022</date>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"119\" to=\"128\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"10,70.87,365.14,218.27,8.64;10,81.78,376.10,209.02,8.64;10,81.78,387.06,209.01,8.64;10,81.78,398.02,208.60,8.64;10,81.78,408.98,208.60,8.64;10,81.47,419.94,208.92,8.64;10,81.78,430.90,209.01,8.64;10,81.78,441.86,209.10,8.64;10,81.78,452.65,207.35,8.81;10,81.36,463.61,207.78,8.58;10,81.42,474.56,207.72,8.81;10,81.78,485.69,122.60,8.64\" xml:id=\"b26\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_4BzGdtz\">Transformers: State-of-the-art natural language processing</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Thomas</forename><surname>Wolf</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Lysandre</forename><surname>Debut</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Victor</forename><surname>Sanh</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Julien</forename><surname>Chaumond</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Clement</forename><surname>Delangue</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Anthony</forename><surname>Moi</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Pierric</forename><surname>Cistac</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Tim</forename><surname>Rault</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Remi</forename><surname>Louf</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Morgan</forename><surname>Funtowicz</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Joe</forename><surname>Davison</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Sam</forename><surname>Shleifer</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Clara</forename><surname>Patrick Von Platen</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Yacine</forename><surname>Ma</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Julien</forename><surname>Jernite</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Canwen</forename><surname>Plu</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Teven</forename><surname>Xu</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Sylvain</forename><surname>Le Scao</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Mariama</forename><surname>Gugger</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Quentin</forename><surname>Drame</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Alexander</forename><surname>Lhoest</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><surname>Rush</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno type=\"DOI\">10.18653/v1/2020.emnlp-demos.6</idno>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_FecnJpw\">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>\\n\\t\\t<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>\\n\\t\\t<imprint>\\n\\t\\t\\t<publisher>Online. Association for Computational Linguistics</publisher>\\n\\t\\t\\t<date type=\"published\" when=\"2020\">2020</date>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"38\" to=\"45\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"10,70.87,506.09,220.01,8.64;10,81.47,517.05,207.67,8.64;10,81.78,527.84,207.36,8.81;10,81.53,538.80,207.61,8.58;10,81.42,549.76,208.96,8.81;10,81.78,560.88,207.36,8.64;10,81.42,571.84,49.53,8.64\" xml:id=\"b27\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_tgJs7Jf\">Transformer-based part-of-speech tagging and lemmatization for Latin</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Krzysztof</forename><surname>Wr\\xc3\\xb3bel</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Krzysztof</forename><surname>Nowak</surname></persName>\\n\\t\\t</author>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_yKh7TTM\">Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages</title>\\n\\t\\t<meeting>the Second Workshop on Language Technologies for Historical and Ancient Languages<address><addrLine>Marseille, France</addrLine></address></meeting>\\n\\t\\t<imprint>\\n\\t\\t\\t<publisher>European Language Resources Association</publisher>\\n\\t\\t\\t<date type=\"published\" when=\"2022\">2022</date>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"193\" to=\"197\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"10,70.87,592.24,218.27,8.64;10,81.78,603.20,209.02,8.64;10,81.78,613.99,209.01,8.81;10,81.78,624.95,209.01,8.58;10,81.78,635.91,207.36,8.58;10,81.53,646.86,208.86,8.81;10,81.78,657.99,207.36,8.64;10,81.78,668.95,46.77,8.64\" xml:id=\"b28\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_CRaER9H\">Named entity annotation projection applied to classical languages</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Chiara</forename><surname>Tariq Yousef</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Gerhard</forename><surname>Palladino</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Stefan</forename><surname>Heyer</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><surname>J\\xc3\\xa4nicke</surname></persName>\\n\\t\\t</author>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_6rfmwEq\">Proceedings of the 7th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage</title>\\n\\t\\t<meeting>the 7th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage<address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>\\n\\t\\t<imprint>\\n\\t\\t\\t<publisher>Association for Computational Linguistics</publisher>\\n\\t\\t\\t<date type=\"published\" when=\"2023\">2023</date>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"175\" to=\"182\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"10,70.87,689.35,220.01,8.64;10,81.78,700.14,209.01,8.81;10,81.78,711.10,91.66,8.58\" xml:id=\"b29\">\\n\\t<monogr>\\n\\t\\t<title level=\"m\" type=\"main\" xml:id=\"_paagDab\">2022a. Transformer-Based Named Entity Recognition for Ancient Greek</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Chiara</forename><surname>Tariq Yousef</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Stefan</forename><surname>Palladino</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><surname>J\\xc3\\xa4nicke</surname></persName>\\n\\t\\t</author>\\n\\t\\t<idno type=\"DOI\">10.13140/RG.2.2.34846.61761</idno>\\n\\t\\t<imprint/>\\n\\t</monogr>\\n</biblStruct>\\n\\n<biblStruct coords=\"10,70.87,731.66,218.27,8.64;10,81.78,742.62,209.01,8.64;10,81.78,753.41,207.36,8.81;10,81.78,764.37,207.35,8.58;10,316.69,75.17,208.96,8.81;10,317.05,86.30,207.36,8.64;10,316.69,97.26,49.53,8.64\" xml:id=\"b30\">\\n\\t<analytic>\\n\\t\\t<title level=\"a\" type=\"main\" xml:id=\"_CqXeMar\">Automatic translation alignment for Ancient Greek and Latin</title>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Chiara</forename><surname>Tariq Yousef</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">David</forename><forename type=\"middle\">J</forename><surname>Palladino</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><forename type=\"first\">Monica</forename><surname>Wright</surname></persName>\\n\\t\\t</author>\\n\\t\\t<author>\\n\\t\\t\\t<persName coords=\"\"><surname>Berti</surname></persName>\\n\\t\\t</author>\\n\\t</analytic>\\n\\t<monogr>\\n\\t\\t<title level=\"m\" xml:id=\"_ktS74Vy\">Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages</title>\\n\\t\\t<meeting>the Second Workshop on Language Technologies for Historical and Ancient Languages<address><addrLine>Marseille, France</addrLine></address></meeting>\\n\\t\\t<imprint>\\n\\t\\t\\t<publisher>European Language Resources Association</publisher>\\n\\t\\t\\t<date type=\"published\" when=\"2022\">2022</date>\\n\\t\\t\\t<biblScope unit=\"page\" from=\"101\" to=\"107\" />\\n\\t\\t</imprint>\\n\\t</monogr>\\n</biblStruct>\\n\\n\\t\\t\\t\\t</listBibl>\\n\\t\\t\\t</div>\\n\\t\\t</back>\\n\\t</text>\\n</TEI>\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsp[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a780a476-4fb7-4161-b3a9-1dd2df4f35e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.xml', 'wb') as f:\n",
    "    f.write(rsp[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3e30dd3-ba0e-4284-8627-2eab0676a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "358e1720-8145-455c-a2b0-181c39535b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('code/code/parse_grobid_extraction/acl_corpus_full-text.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b132a358-c9af-404e-b5ac-0fde71aa0162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>Training and Evaluation of Named Entity Recogn...</td>\n",
       "      <td>We evaluate the performance of various models ...</td>\n",
       "      <td>We evaluate the performance of various models ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  acl_id                                              title  \\\n",
       "0   test  Training and Evaluation of Named Entity Recogn...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  We evaluate the performance of various models ...   \n",
       "\n",
       "                                           full_text  \n",
       "0  We evaluate the performance of various models ...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e45db6d-ff7c-4cfd-a883-0536682ddd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = df['full_text'][0]\n",
    "with open('out.txt', 'w') as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86109365-45c3-4332-8fb9-9ecada2e3f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
